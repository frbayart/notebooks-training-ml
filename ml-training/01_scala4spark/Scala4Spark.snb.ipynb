{
  "metadata" : {
    "id" : "26d1a460-fe0e-4e72-a49d-d63a09b749f2",
    "name" : "Scala4Spark.snb.ipynb",
    "user_save_timestamp" : "2018-02-27T07:38:31.612Z",
    "auto_save_timestamp" : "2015-01-10T00:02:12.659Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "4E44922AC336459281C6A17D2BEEE8D3"
    },
    "cell_type" : "markdown",
    "source" : "<style>\n  h1, h2, h3, h4, h5, p, ul, li {\n    color: #2C475C;\n  }\n  .output_html {\n    color: skyblue;\n  }\n  hr { height: 2px; color: lightblue; }\n</style>"
  }, {
    "metadata" : {
      "id" : "52FFAEEE323B4F4083216A001E8CEC14"
    },
    "cell_type" : "markdown",
    "source" : "# An overview of Scala and Spark"
  }, {
    "metadata" : {
      "id" : "0820C88DD7B846FA84645A3FF7D379C5"
    },
    "cell_type" : "markdown",
    "source" : "## Introduction"
  }, {
    "metadata" : {
      "id" : "E3311D3EADD145558AD9FEB3DD0A77A1"
    },
    "cell_type" : "markdown",
    "source" : "In this chapter we will cover a simple data processing case to give an overview of what Scala and Spark are bringing on the table as a language and data analytics framework.\n\nWe will take a sample of stock market end-of-day data, compute some simple transformations and make joins. This will allow to preview some of the valuable features of scala and hint on how spark is working."
  }, {
    "metadata" : {
      "id" : "020B5090A7CA4A1A91BF81613C1737BC"
    },
    "cell_type" : "markdown",
    "source" : "## Prelude"
  }, {
    "metadata" : {
      "id" : "F85F75BA2EE7456A80FE2139318F871B"
    },
    "cell_type" : "markdown",
    "source" : "Outside the Spark Notebook environment a list of imports is necessary to get every needed properly loaded."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8651340A2B494804AF2A5DE5E58E49E6"
    },
    "cell_type" : "code",
    "source" : [ "import org.apache.spark._\n", "import org.apache.spark.rdd._\n", "import org.apache.spark.sql._\n", "import org.apache.spark.sql.SparkSession._\n", "import org.apache.spark.SparkContext._" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "8252ACCDE6C94CD38A14ECDFF7727F61"
    },
    "cell_type" : "markdown",
    "source" : "## Download some stock market data\n\nWe start with downloading some data to work with. We call the Yahoo Finance Historical data service, with a simple http query to get csv files containing the requested data, _i.e._ end of day quotes for a selected stock or other financial instrument."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "0B7AFD4FA9BF469D845256C77B29B2A1"
    },
    "cell_type" : "markdown",
    "source" : "We will download the data using the native `scala.io.Source` API, there are other more specific ways to access the internet, using for instance the `sys.process.ProcessBuilder` to do a system call to `curl` or `get` for instance.\n\nBefore that, we set a variable `dataDir` as our base data directory, here by appending `/data/scala4spark` to the `TMP` directory. We will discuss `val` variables and types in a few more instructions."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E64FE53836B04B1381CA46BEE571F9C4"
    },
    "cell_type" : "code",
    "source" : [ "val dataDir = sys.props(\"java.io.tmpdir\") + \"/data/scala4spark\"" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "62A6B2D73263441199B83DAAEDD21B8E"
    },
    "cell_type" : "markdown",
    "source" : "We make sure the folder exists."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9CBE642718194ECDA2C065989EE09C9D"
    },
    "cell_type" : "code",
    "source" : [ "new java.io.File(dataDir).mkdirs()" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "5297ABC766C3460E83E0675B00F1F849"
    },
    "cell_type" : "markdown",
    "source" : "In the following, we creating an helper function to download the online data as _CSV_ (Comma Separated Values) files."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9904BE17AC9A407889D08BC7B10A410F"
    },
    "cell_type" : "code",
    "source" : [ "val base = \"https://s3-eu-west-1.amazonaws.com/kensuio-training/data\"\n", "def mkUrl(symbol: String) = s\"${base}/${symbol}-2017.csv\"" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "083816DB05CF48748CA322FB50C43C20"
    },
    "cell_type" : "code",
    "source" : [ "def dl(q:String) {\n", "  val source = scala.io.Source.fromURL(mkUrl(q))\n", "  val f = new java.io.FileWriter(new java.io.File(s\"${dataDir}/$q.csv\"), false)\n", "  source.foreach(f.append(_))\n", "  f.close\n", "}" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "96CB0FADAF734F96936F73F870F2370D"
    },
    "cell_type" : "markdown",
    "source" : "Quite a few things to comment on for these instructions. The strings are preceeded with `s`. `s` is a function applied on the string to interpolate its content. This means that the dollar sign and curly braces can be used to define scala expression to be evaluated within the String. \n\nFor exemple `${qparam}` is replaced by the value of the `qparam` variable, same goes for the dataDir variable.\n\nThen use a combination of Scala API and Java API to read the online data (`Source`) and write it into a local file (`FileWriter`)"
  }, {
    "metadata" : {
      "id" : "B8A7EB21C33F47B1B4251CAFDCC8A898"
    },
    "cell_type" : "markdown",
    "source" : "We can now use the helper to download the data we'll use the remaining of the book."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2D0137BE5E41421B84EDFAD3929C1BAA"
    },
    "cell_type" : "code",
    "source" : [ "dl(\"GOLD\")\n", "dl(\"COP\")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "30341AD2EC204DBD89DC2B2423AE4CF9"
    },
    "cell_type" : "markdown",
    "source" : "## Read data with Spark\n\nNow that some data is available on the file system, we can start working on it. The first obvious thing to do is to check the content, by looking at a few lines in the file.\nWe start with opening the file with Spark:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1FD484647EFA440693CE49A59EEE6D6B"
    },
    "cell_type" : "code",
    "source" : [ "val oiltxt: RDD[String] = sparkContext.textFile(s\"$dataDir/COP.csv\")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "5AC46C7A599546AB9F9AEE38D2378443"
    },
    "cell_type" : "markdown",
    "source" : "Here, there again quite a few things to note. \n\n`oiltxt` is a variable declared as `val`. A `val` is an immutable variable, it must be initialized at creation and it is not possible to change its value. Using immutable variables is a very important characteristic of functional programming like in Scala. Using immutable variables removes the temptation of spreading side effects in scopes that are not well constrained and thus is much less error prone and easier to test.\n\nThe type of the `oiltxt` variable is `RDD[String]`. `RDD` stands for Resilient Distributed Dataset. It is a Spark abstraction over a collection of items, here of type `String` type. This abstraction allows to define computations on this collection of items, for execution in the distributed environment.\n\nWe explicitly set the variable type here, but the compiler doesn't need it to be set by the developer. The `textFile` function returns a `RDD[String]` type thus the following would be valid:\n\n```scala\n\nval oiltxt = sparkContext.textFile(\"OIL.csv\")\n\n```\n\n`sparkContext` is a variable defined by Spark. It is the gateway to the distributed environment, it contains the configuration for connecting the cluster, and as you already understood, it is used to  access stored data in the distrubuted environment. It is in charge of building the DAG (Directed Acyclic Graph) defining the successions of commputations. It is the DAG that allows resiliency because a computation plan is kept to reconstruct data in case of failure. The SparkContext also schedules computations on the cluster.\n \n`textFile` is a function to read some text data, every single line becomes one item of the collection represented by the `RDD`.\n \nExecution of this line instantiate the `oiltxt` variable and we have confirmation of its type, `RDD[String]`.\n\nAt this point, we have an `RDD` which defines one option: readling lines from a file. But, this optionration is not executed, only its definition is created. If we take a look at the User Interface provided by Spark to check the computations it evaluates, then we'll notice that it actually did nothing. Actually, there is no computations scheduled yet. The Spark UI is generally available on the port `4040` of the host running the program. Using the Spark Notebook you can use the menu entry `View > Open Spark UI`."
  }, {
    "metadata" : {
      "id" : "380639ADFE2141559A53C4AB4068E853"
    },
    "cell_type" : "markdown",
    "source" : "> **Distributed (Partitioned) Data** (cf Chapter 07)\n>\n> The file are certainly used locally in this example.\n>\n> However nowadays, it is a good exercise while processing data to always make the assertion it is distributed. In fact, the distribution of the dataset is really easy to understand. The data is split into a number of files without intersection stored in different machines, hence their name `Partition`.\n>\n> This is why, Spark, by default will always split any data in at least two (see `spark.default.parallelism`) partitions based."
  }, {
    "metadata" : {
      "id" : "7C8623A03F6E444D8FCF82D9C3BF1816"
    },
    "cell_type" : "markdown",
    "source" : "## Actions"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C3A04FBC83ED4B2490CD69B82D1E23E4"
    },
    "cell_type" : "code",
    "source" : [ "oiltxt.count" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "9CCE78197BFE471A889C378D54568A9E"
    },
    "cell_type" : "markdown",
    "source" : "We downloaded one year of daily data, so 255 points make sense. In order to compute this, the data must be read thus the partitions need to be accessed in the cluster. Looking again at the Spark UI, we have this confirmation.\n\nA single stage was executed, with two step: reading data and counting items. The number of tasks in this stage depend on the number of partitions that were processed, in local and by default it equals to the number of cores on which you are running."
  }, {
    "metadata" : {
      "id" : "79D1F52AADDC40269819B641A72C137D"
    },
    "cell_type" : "markdown",
    "source" : "Let's take a look at the content of the data file, for instance taking the first three lines and display their content (as `String`s)."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab603981592-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "4B5829FE64894D8A8FCED65F3E6AF31C"
    },
    "cell_type" : "code",
    "source" : [ "oiltxt.take(3)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "5BB938D1BF044D368E8C4B54DFB3BFC1"
    },
    "cell_type" : "markdown",
    "source" : "The `take(n: Int)` function is also an action, it triggers computations, we can confirm this looking the Spark UI. In this case, there is only one task, because in order to take the first 3 lines, we only need to process one partition of the data, while we needed the entire dataset to count the total number of items. \n\nFrom the distributed dataset, `take(n: Int)` takes the first lines and collects them on the driver (which is running the current code), as an array of Strings (`Array[String]`)."
  }, {
    "metadata" : {
      "id" : "9505FDBC8654463BA4321CAAC3224ACF"
    },
    "cell_type" : "markdown",
    "source" : "## Filtering data\n\nWe have the first line as header and following lines containing the data. We want to do some computations on the numerical values, for this we need to remove the header line. We can do this with a filter:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1935042548-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "70D0B5F6850F472C88197E49716B6310"
    },
    "cell_type" : "code",
    "source" : [ "oiltxt.filter( (s: String) => ! s.startsWith(\"Date\"))" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "ADB018EF182A4C378DB5C1F9035D8BBF"
    },
    "cell_type" : "markdown",
    "source" : "We called the `filter` function on the `oiltxt` RDD. This function takes an argument which may look \"funny\" if not cryptic. This argument is actually an anonymous function or lambda. \n\n`s` is the function argument and the function body comes after the `=>`. So here the function takes one `String` argument and returns a `Boolean`, if the `String` doesn't start with `\"Date\"`.\n\nThis function will be applied on each element of the `oiltxt` RDD, using the item as the function argument. So on each item, `s` will be take the line of text, those lines to satistfy the predicate are kept in the resulting `RDD`, the others are discarted.\n\nNote we define the operations to be applied on every elements, but we doen't tell anything about how to loop over the elements. Also we don't create the resulting `RDD`. Furthermore, we are not specifying if the transformation is executed sequentially (one core), in parallel (several cores) or distributed (cluster), actually the same code applies to all mode when using Spark.\n\nThis is one of the advantages of applying the functional programming, the separation between:\n\n* defining computations and \n* the execution plan (scheduling).\n\nAgain, the `filter` returns an `RDD` and this actually means that it is lazy, as per `textFile`. Hence, to know the result we need to run an action like `count` or `take` to trigger the computation and see results on the driver."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "738E053D96234B298AE61CBD5A2402EA"
    },
    "cell_type" : "code",
    "source" : [ "oiltxt.filter( s => ! s.startsWith(\"Date\") ).count" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "7B32F0ADE8C5473C8CC0AD8CC2959A14"
    },
    "cell_type" : "markdown",
    "source" : "As expected, we removed only one element, as there is only 1 header line starting with `\"Date\"`. \n\n## Parsing data\n\nNow we will parse the lines to extract some numerical values, for instance the last field which is the close price of the stock:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab270859451-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "5846971AA5894FA9A0FE719A99378F0D"
    },
    "cell_type" : "code",
    "source" : [ "oiltxt.filter( s => ! s.startsWith(\"Date\") )\n", ".map( s => {\n", "  val arr = s.split(\",\")\n", "  arr(arr.size - 2).toDouble // last element is volume\n", "}).first" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "3305FC7D0AE247B68D1DE927C4890304"
    },
    "cell_type" : "markdown",
    "source" : "This is a bit more involved, the `map` function takes itself a function like `filter` as argument which is applied on each element of the `RDD`.\n\nAfter filtering out the header line, we apply a function on each element to split the line using the \",\" as separator, this returns an `Array[String]` from wich we take the `last` element and convert to `Double`. Again, very straigthforward.\n\nThe result is an `RDD` with `Double` as elements (`RDD[Double]`) from which we extract the date as a `String` and price and and build an `RDD` of Tuples:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "716E68FAF20546469E35B5D34F390EDA"
    },
    "cell_type" : "code",
    "source" : [ "" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9E611403346F4260B12DF4F67DC9AD78"
    },
    "cell_type" : "code",
    "source" : [ "val oil = oiltxt.filter( s => ! s.startsWith(\"Date\") )\n", "                .map{ s =>\n", "                  val tokens = s.split(\",\")\n", "                  (tokens(1), tokens(tokens.size - 2).toDouble)\n", "                }" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "382AD4DA5D264043B8A55CC092D0D92D"
    },
    "cell_type" : "markdown",
    "source" : "Tuples are scala structures used to hold a number of elements of different types, in this case, two elements, a `String` and a `Double`. The type is `(String, Double)`.\n\nElements of a tuple can be accessed using `_1` (`_2`, `_3`, ...) for the first (second, third, ...resp) element. \n\nFor example, one way to get and RDD of dates would be:\n\n```scala\n\noil.map( t => t._1 )\n\n```\n\nor\n\n```scala\n\noil.map( _._1 )\n\n```\n\nHere, because we use the function parameter (`t`) only once, in Scala we can then replace it with the _ placeholder. Because it cannot be mistaken we don't need to explicitely set a variable name. This writing is more compact and still very clear. We will see some examples with two such placeholders (two arguments, each used only once) later."
  }, {
    "metadata" : {
      "id" : "9B7A3D643BBF44B68CFD6834B561E17F"
    },
    "cell_type" : "markdown",
    "source" : "## Grouping elements\n\n\nThe next thing we will do is to group data to build an histogram of the price distribution. We will compute the mean and standard deviation of the prices using the `mean` and `stdev` functions of spark on `RDD[Double]`."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9C4952C0A69648318703946E7F25CFE0"
    },
    "cell_type" : "code",
    "source" : [ "val meanoil = oil.map( _._2 ).mean\n", "val sdoil   = oil.map( _._2 ).stdev" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "A3418DAED7E74B49AAF15E0008711BA6"
    },
    "cell_type" : "markdown",
    "source" : "Then we group data by computing `z`, the distance to mean price in standard deviation units and we round that number:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab65660832-3\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "17D8290919F34F02AD7DDC19298D38C9"
    },
    "cell_type" : "code",
    "source" : [ "oil.groupBy( t => ((t._2 - meanoil)/sdoil).round )" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B96B24D96BC94C478C5762BA73314B8A"
    },
    "cell_type" : "markdown",
    "source" : "The type of the tuples in the `RDD` is `(Long, Iterable[(String, Double)])`. The first element ( `Long` ) is the computed rounded distance to average. It is the grouping key. The values corresponding to this grouping keys are accessible as an `Iterable[(String, Double)])`. Each element of this iterable being one element of the original `RDD[(String, Double)]`.\n\nUsing a strongly typed language is really helping us to understand what we are manipulating and how a function is transforming our data. The compiler provides us direct feedback on what we are doing and helps to pinpoint errors without running the computations on the data.\n\nFrom this point,o we can do more in order to get the histogram data, we just have to count the number of elements in each group. A `mapValues` function allows us to apply a function on each value (and Iterator here):"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "54EF65AB80CF4BFC8382605DA6898CDE"
    },
    "cell_type" : "code",
    "source" : [ "oil.groupBy( t => ((t._2 - meanoil)/sdoil).round )\n", "   .mapValues( _.size )" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "CD690E94533D420087A139B671FC2876"
    },
    "cell_type" : "markdown",
    "source" : "Now items are of type `(Long, Int)`, the element of the tuple being the distance to mean and the second element is the size of the group.\n\nWe then need to collect that data and sort using the key:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1707675537-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "29B3B9F343654EB4B667094024B072ED"
    },
    "cell_type" : "code",
    "source" : [ "TableChart(\n", "  oil.groupBy( t => ((t._2 - meanoil)/sdoil).round )\n", "   .mapValues( _.size )\n", "   .collect\n", "   .sortBy( _._1 )\n", ")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "64B0FD6E145C4BDC86A360485D9A4DFC"
    },
    "cell_type" : "markdown",
    "source" : "You see how we have just constructed, step by step some data transformation, looking at the types returned at each new instruction to make sure we were computing the expected quantities. This is one of the great value in strongly typed languages using with a REPL or notebook. "
  }, {
    "metadata" : {
      "id" : "4CC9B0DC9EBD442DBD2A67C5E0CA4130"
    },
    "cell_type" : "markdown",
    "source" : "## Joins"
  }, {
    "metadata" : {
      "id" : "A08E22E0CE6249F5951C0F7BE4D31D7F"
    },
    "cell_type" : "markdown",
    "source" : "Now we will read and parse the other file with GOLD (GLD) data and join data on the date, to be able to compare GOLD and OIL prices. This is exacly the same functions as for OIL data:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0E19A2B8BDE4490EBC5852FD3EEC716D"
    },
    "cell_type" : "code",
    "source" : [ "val gld = sparkContext.textFile(s\"$dataDir/GOLD.csv\")\n", "                      .filter( s => ! s.startsWith(\"Date\"))\n", "                      .map{ s =>\n", "                  val tokens = s.split(\",\")\n", "                  (tokens(1), tokens(tokens.size - 2).toDouble)\n", "                      }" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "D9EC1A87A0EF49FDA34C0CF9E5F8506D"
    },
    "cell_type" : "markdown",
    "source" : "We will compute the GLD prices mean and standdard deviation for scaling purposes:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0B74E4905842421BA18324CF31D59C12"
    },
    "cell_type" : "code",
    "source" : [ "val meangold = gld.values.mean\n", "val sdgold = gld.values.stdev" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "2CAE3B7EC3834C3594C81C568A4A8E55"
    },
    "cell_type" : "markdown",
    "source" : "When a tuple has two elements, the first one is called the key and the second one is the value. If two `RDD`s have tuples as elements with keys of the same type, then it is possible to join them. Like here:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8792EF18347F4EA483EF79D8E5DCF124"
    },
    "cell_type" : "code",
    "source" : [ "val oilgld = oil.join(gld)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B3429653283A4AB78BD563DB6356A629"
    },
    "cell_type" : "code",
    "source" : [ "oilgld.first" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "8B5056FA07134F3CBEB6C03DBED3935E"
    },
    "cell_type" : "markdown",
    "source" : "The resulting `RDD` has elements of type `(String, (Double, Double))`. \n\nThe key is the common `String`, and the value is a tuple containing the `Double` for OIL and GLD."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1848077300-1\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [\n    \"_2\"\n  ],\n  \"rows\": [],\n  \"vals\": [\n    \"_1\"\n  ],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Average\",\n  \"rendererName\": \"Scatter Chart\"\n}"
      },
      "id" : "B4EB7C471AE14DAA8C3BB50EF90C3857"
    },
    "cell_type" : "markdown",
    "source" : "We can now collect this data and scale the prices to see a comparision of GOLD and OIL price:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab853668890-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "1B160390FC3045A980A7EDDDA3C28310"
    },
    "cell_type" : "code",
    "source" : [ "ScatterChart(\n", "  oilgld.mapValues( t => (t._1 - meanoil, t._2 - meangold))\n", "        .values\n", "        .sortBy(_._1)\n", "        .collect\n", ")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "22E2A053074A4F638E7CB3AF8184597E"
    },
    "cell_type" : "markdown",
    "source" : "## Spark DAG and resilience\n\nAll these computations and the join is done on distributed datasets. \n\nHow are they scheduled? We have seen (in <a href=\"#Actions\">Actions</a> that computations are only triggered when an action is called, ususally requiring data to come back on the driver (collect, take, count, or writing outputs).\n\nSo a Direct Acyclic Graph (DAG) of computations is constructed and used to plan the execution of all computations when required. \n\nHere is how to have information about this DAG:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8A98B631DD3F4045B2EAE374D8A8CA30"
    },
    "cell_type" : "code",
    "source" : [ "println(oilgld.toDebugString)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "EC67A108B9BA47168FC70BE2D58CBA9E"
    },
    "cell_type" : "markdown",
    "source" : "We clearly see here that two data sets are read, filtered and mapped before being joined. The complete set of computations is defined this way. If any failure occurred at the level of a computing node, it would be possible to reconstruct the data because the plan remains and the resiliency is guaranteed.\n\nYou find the same DAG in a more user friendly representation in the Spark UI."
  }, {
    "metadata" : {
      "id" : "1D7E380317264799BE29A945CFADE5DC"
    },
    "cell_type" : "markdown",
    "source" : "## Caching\n\nWe will now work on a bigger dataset and see how working eplicitely with memory (caching) can help with performance. We will prepare a dataset and use it several times.\n\nFor this we will define our own class instead of tuples, because using our own class definition  fields accesses easier than with tuples."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E2996B898DEE4D278B8DCB4FEAE7EA78"
    },
    "cell_type" : "code",
    "source" : [ "case class Quote(symbol:String, date:String, price:Double)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "1149A328139347DB8472F1080E61D677"
    },
    "cell_type" : "markdown",
    "source" : "So we defined here a `case` class. Case classes are much like a Java class, except that they come with a set of very interesting features. The case class signature is also the contructor signature, and accessors to the fields are predefined for us. The instantiation and use of case classes is much easier than vanilla Scala classes. We will see this in action in a minute."
  }, {
    "metadata" : {
      "id" : "E6C26345B785475CB01F864B7326E0D8"
    },
    "cell_type" : "markdown",
    "source" : "We will read data from a file containing lines like:\n``` \nASTE,2011-12-06,33.93\nASTE,2012-03-14,36.84\n```\n\nwhere the `(symbol, date, price)` fields of the `Quote` case class are clearly identified.\n\nWe start with a download of the files we prepared for you:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2F49D004DBCD42F79A22F4D2B65A6D40"
    },
    "cell_type" : "code",
    "source" : [ "if (!new java.io.File(s\"${dataDir}/closes.csv\").exists) {\n", "  val source = scala.io.Source.fromURL(s\"https://s3-eu-west-1.amazonaws.com/spark-notebook-data/closes.csv\")\n", "  val f = new java.io.FileWriter(new java.io.File(s\"${dataDir}/closes.csv\"), false)\n", "  source.foreach(f.append(_))\n", "  f.close\n", "}" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "BA1CC25397A2449EBF27627C12F364F8"
    },
    "cell_type" : "markdown",
    "source" : "Then we will read the file and parse it:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8A5EC3C84E81471A8CFE5D46D6518329"
    },
    "cell_type" : "code",
    "source" : [ "val closes:RDD[Quote] = sparkContext.textFile(s\"${dataDir}/closes.csv\")\n", "                                   .map(_.split(\",\").toList)\n", "                                   .map{ case List(s, d, p) => Quote(s, d, p.toDouble)}" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "2C13785C455C45D48B6E1BAF739CC24F"
    },
    "cell_type" : "markdown",
    "source" : "Our resulting `RDD` is of type `RDD[Quote]` as expected."
  }, {
    "metadata" : {
      "id" : "154002ACDB174762A7B5352BD1D803F3"
    },
    "cell_type" : "markdown",
    "source" : "We will group stock prices per day:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F8EC68E867AD4CAB8C4A3146277093D3"
    },
    "cell_type" : "code",
    "source" : [ "val byDate:RDD[(String, Quote)] = closes.keyBy(_.date)" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "1A2220763E97427B8DA77A28E9A8F5EF"
    },
    "cell_type" : "markdown",
    "source" : "You see how we call the accessor function on the `date` field from the `Quote` case class.\n\nThe `keyBy` function takes another function telling how to extract a key of each element, thus each element is turned into a tuple where the first element is the extracted key, and the second element is the original element. \n\n> Note that these objects are used as immutable, changing a value passes by the creation of new instance."
  }, {
    "metadata" : {
      "id" : "BEF2305328B149C1AB0CB085BCC9076D"
    },
    "cell_type" : "markdown",
    "source" : "Now we can compute the minimum stocks price per date. To do this we need a function somewhat more sophisticated. without yet entering the details, the idea is that we want to work with symbols and prices for a given date, by selecting the lowest price:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "D91C37B2E0544C588007E6FE72DE9D00"
    },
    "cell_type" : "code",
    "source" : [ "def minByDate = byDate.groupByKey.mapValues(quotes => quotes.minBy(_.price))" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "22B355C074AD4A559903F903A628CBA9"
    },
    "cell_type" : "code",
    "source" : [ "def minByDate = byDate.combineByKey[(String, Double)](                                                                                           // `def` to force spark recomputing... otherwise it's smart enough to reuse previous RDDs...\n", "  (x:Quote) => (x.symbol, x.price), \n", "  (d:(String, Double), l:Quote) => \n", "    if (d._2 < l.price) d else (l.symbol, l.price),\n", "  (d1:(String, Double), d2:(String, Double)) => if (d1._2 < d2._2) d1 else d2\n", ")" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "A4865FEA54AD4E658F9E52C42B4B9377"
    },
    "cell_type" : "markdown",
    "source" : "`minByDate` is a function definition without argument list such that the computation will be executed at each call. \n\nIt returns an `RDD` of tuple with a date as key `(String)` and the `(symbol, price)` pair as value.\n\nNow we can trigger the computation:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "4C54137EDAF04543A485814E824190B9"
    },
    "cell_type" : "code",
    "source" : [ "<pre>{minByDate.take(2).toList.mkString(\"\\n\")}</pre>" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "CFABEA44631549B59284C9668AE414AF"
    },
    "cell_type" : "markdown",
    "source" : "This computation takes a bit of time. If we do this again:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "C4003494928F438D8AA14BC6D0FC90CB"
    },
    "cell_type" : "code",
    "source" : [ "<pre>{minByDate.take(2).toList.mkString(\"\\n\")}</pre>" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "D77913C698DF4372B84B1B29AA9252DA"
    },
    "cell_type" : "markdown",
    "source" : "The computation time is sensibly the same because the we need to access the data from the disk and to recompute the steps in the DAG each time.\n\nThe solution is to cache in memory the result of the computation stages."
  }, {
    "metadata" : {
      "id" : "2FC6A83752AE415A9A5A286E05798ED6"
    },
    "cell_type" : "markdown",
    "source" : "We define here another version of the combiner to compute the maximum stock price per day. This time we store the result in a variable and explicitly cache it. The resulting RDD, once computed in the cluster will be stored in memory as much as possible."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "213FCBFE5B0740AAB3D7584FD9E77B30"
    },
    "cell_type" : "code",
    "source" : [ "val maxByDate = byDate.groupByKey.mapValues(quotes => quotes.minBy(_.price)).cache()" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "3E36DAA434F842FD8D879050B6A1443A"
    },
    "cell_type" : "markdown",
    "source" : "We trigger the computation, as before:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "4B2060DFBDE3414F846B122A523E0217"
    },
    "cell_type" : "code",
    "source" : [ "<pre>{maxByDate.take(2).toList.mkString(\"\\n\")}</pre>" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "8FC9D842BF8948628473EBE1D8018B7D"
    },
    "cell_type" : "markdown",
    "source" : "The computation time is not better, because it is the first computation. But now, if we look at the Spark UI under the `storage`tab, we can see that the data is stored in memory. \n\nWe can now expect a much faster access to this result:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "CEB5539AD3EF4AE8818B20BA0F55A83F"
    },
    "cell_type" : "code",
    "source" : [ "<pre>{maxByDate.take(2).toList.mkString(\"\\n\")}</pre>" ],
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "C4AE8792155A4C12B1FC975D782DEF4C"
    },
    "cell_type" : "markdown",
    "source" : "Indeed the results are obtained in a small fraction of the time."
  }, {
    "metadata" : {
      "id" : "9D64E08F155B402F9C9E91674941AFB1"
    },
    "cell_type" : "markdown",
    "source" : "## Takeovers\n\nWe have already covered quite a lot of things in this chapter. The idea was to show some real thing done with scala and spark, showing some of the important feartures of the language as well as some features of spark and how it is working. You don't need to feel comfortable with everithing at this stage. All of this will be coverted in details all along the book but you have already a good overview of what spark and scala can do."
  } ],
  "nbformat" : 4
}