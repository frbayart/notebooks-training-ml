{"metadata":{"id":"0524a50d-f8d4-4aba-8e6f-29b9b23af41e","name":"Variance - Bias","user_save_timestamp":"1970-01-01T01:00:00.000Z","auto_save_timestamp":"1970-01-01T01:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"sparkNotebook":null,"customLocalRepo":"/tmp/spark-notebook","customRepos":null,"customDeps":["com.cra.figaro %% figaro % 2.2.2.0"],"customImports":null,"customArgs":null,"customSparkConf":null,"customVars":null},"cells":[{"metadata":{"id":"19B4E33C0B194D48BA5D0F975BD81807"},"cell_type":"markdown","source":"# Generate some data with a simple model"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"3D3E68F08DA14230BB01987B9316EFA8"},"cell_type":"code","source":["import org.apache.spark.rdd.RDD\n","import com.cra.figaro.library.atomic.continuous._"],"outputs":[]},{"metadata":{"id":"A3BFEF8719D5447491D2845268B59C89"},"cell_type":"markdown","source":"## System generating X values"},{"metadata":{"id":"6B243002E2E042778DA5B6609535778E"},"cell_type":"markdown","source":"The first variable is $X \\sim \\textit{U}(1.0, 5.0)$"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"A93E1CC4A30D433B86406BE8F0824253"},"cell_type":"code","source":["val generatorModelX = Uniform(1.0, 5.0)\n","\n","val generateOneX:()=>Double = () => {\n","  generatorModelX.generate()\n","  generatorModelX.value\n","}"],"outputs":[]},{"metadata":{"id":"28E43782BFEB46A685C56686F62FC193"},"cell_type":"markdown","source":"Here what values $X$ will take randomly:"},{"metadata":{"id":"790868EC94B445358532A46D8BFCB6ED"},"cell_type":"markdown","source":"First, we generate the values for the $x$ axis, which will be simply the list from $1$ to $1000$."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"FCB668D829A84F5D8247428ABFD36B62"},"cell_type":"code","source":["val xAxis = List.range(1, 1000, 1)"],"outputs":[]},{"metadata":{"id":"E58D534A55674CF98F9C5CEECA398AFC"},"cell_type":"markdown","source":"Then we can generate the list of values for each value taken by $X$ on the `x` axis (well... for  $U$ it foesn't matter though)"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"13B0789B90864B9284166DF8DF4746F6"},"cell_type":"code","source":["val yAxis = xAxis.map(x => generateOneX())"],"outputs":[]},{"metadata":{"id":"405D00B6F7CB437DA4F645421AB8C454"},"cell_type":"markdown","source":"Now we can plot the values and _see_ how the distribution looks like.\n\nWe'll be using `zip` on the list container, whith create a list of pairs from two given lists, \nsee [`zip`](https://www.scala-lang.org/api/current/scala/collection/immutable/List.html#zip[B](that:scala.collection.GenIterable[B]):List[(A,B)])\n\n```scala\n[1, 2] zip [3, 4] = [ [1, 3] , [2, 4]Â ]\n```\n\nAlso, we will use the plotting functionnality of the Spark Notebook, in this case: the `ListChart` which plots the provided list of pairs as a line."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"80BF20C4D5F3466986A193CD1727B53F"},"cell_type":"code","source":["LineChart(xAxis zip yAxis)"],"outputs":[]},{"metadata":{"id":"32F7F5343C34476487E805274AB0BAA5"},"cell_type":"markdown","source":"As we can see, the values don't show a specific pattern but all values land between $1$ and $5$."},{"metadata":{"id":"6D1E0FE0C7794D2689175BC5A1D71F79"},"cell_type":"markdown","source":"## System generating Y values from X"},{"metadata":{"id":"6989BBF4712249A9AAD6B0CB160869F0"},"cell_type":"markdown","source":"We're going to generate a model like this \n\n$Y = X^2 + Z$\n\n$Z \\sim \\mathcal{N}(0.0, 0.25)$"},{"metadata":{"id":"D59043601C0B4EE9833AD805248857AC"},"cell_type":"markdown","source":"So $Y$ takes its values as simply as the _square_ value of the $X$ values, however there is some more noise added to it using $Z$.\n\n$Z$ can be problematic (and is always there) because it contains (generally) all non considered infomation which can or are not modelled.\nThere are several reasons, generally because they are almost neglectable and not predictable... however, this is not always to be taken as granted; it's always recommended to check your assumptions!"},{"metadata":{"id":"20635B9B70FC4FA0B789EE88F64CEF4E"},"cell_type":"markdown","source":"The noise $Z$ (let's put it like this) is generated with small value around $0$ and deviating slowly (rarely) between $\\pm$ 0.75 "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"47189A3490514B018D526F23BE10BE29"},"cell_type":"code","source":["val generatorModelZ = Normal(0.0, 0.25)"],"outputs":[]},{"metadata":{"id":"5F2C6D34F91B453A849461512B6D22F8"},"cell_type":"markdown","source":"Let's look at the values it will take, using the same process as before."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"0C280E242FB54CFFA0C38EA88BAFEBD9"},"cell_type":"code","source":["val yAxisForZ = xAxis.map(x => generatorModelZ.generateRandomness) "],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"7A25182B5A8442919526529A7B09EA1D"},"cell_type":"code","source":["LineChart(xAxis zip yAxisForZ)"],"outputs":[]},{"metadata":{"id":"B3F8FFF911EF42F18CA3444F2E2986AF"},"cell_type":"markdown","source":"It kind of looks the same as for the $U$ function, however the variation seems to be less, well, uniform.\n\nTo have another view of these value, we can look at the \"density\" by simply binning the values (on the $y$ axis); a simple way to do this is to plot the histogram with the `Plotly` library."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"81A903CD8972465A9BD3B55D93F8D5A3"},"cell_type":"code","source":["CustomPlotlyChart(yAxisForZ, dataOptions=\"{type: 'histogram'}\", dataSources=\"{x: '_2'}\")"],"outputs":[]},{"metadata":{"id":"816C07956B37485785C2F469763CBF24"},"cell_type":"markdown","source":"Another good way to look at a random variable is to use the `BoxPlot` which shows the _quantiles_ and extreme values."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"593D7AFB81C34909837DB7C5FA9CB9E5"},"cell_type":"code","source":["CustomPlotlyChart(yAxisForZ, \n","                  dataOptions=\"{type: 'box'}\",\n","                  dataSources=\"{y: '_2'}\")"],"outputs":[]},{"metadata":{"id":"37A57E24F1794C65A3A478E44CFF8963"},"cell_type":"markdown","source":"As we can see in the above plot, the distribution looks like a \"bell\" curve where most values are around the center, here $0$.\n\nWe'll see in other materials these concept in more details."},{"metadata":{"id":"AAD533F83631461396D5235E9AC5B48F"},"cell_type":"markdown","source":"Now, we can create a function that will generate a value for $Y$ following the model."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"EB579850DF454A2784FF6CD29E60B71B"},"cell_type":"code","source":["val generateOneY = (x: Double) => {\n","  x*x + math.abs(generatorModelZ.generateRandomness) // X^2 + Z\n","}"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"F6D005A44A5F4E04B7F44EFC957425BA"},"cell_type":"markdown","source":"## Generate the universe (1000 items)"},{"metadata":{"id":"DCD389AF2BB84A258739D032F0543C45"},"cell_type":"markdown","source":"An universe is actually **all data**, we never have access to that."},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false,"id":"6D3A4083CAA94C92A4478D01BB7B172B"},"cell_type":"code","source":["val collection = xAxis.map(i => {\n","                   val x = generateOneX().toDouble\n","                   val y = generateOneY(x).toDouble\n","                   (x, y)\n","  })\n","LineChart(collection.toList, maxPoints=collection.size)"],"outputs":[]},{"metadata":{"id":"51A39C35AF0D46D7803607F464FFAB99"},"cell_type":"markdown","source":"Since we'll be using Spark for further investigation with this data, we'll Sparkify the created dataset, `collection`."},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false,"id":"E8AD8E8CA01F4799AB424248285F1920"},"cell_type":"code","source":["val universe = sparkContext.parallelize( collection )\n","universe.cache()\n","universe.count"],"outputs":[]},{"metadata":{"id":"E0CD7483FE6E479498DA366F53288CF4"},"cell_type":"markdown","source":"# Create the model generators"},{"metadata":{"id":"E55EC3DF8D8E417CB64FEA04DE74B322"},"cell_type":"markdown","source":"Create some functions (formulas) to be considered as models. \n\nThat is, they are trying to model the real universe, we know that the real world is actually following a second order distribution, however here we create four models, from order `0` (constant) to to third order.\n\n* $y=intercept$\n* $y=intercept + ax$\n* $y=intercept + ax + bx^2$\n* $y=intercept + ax + bx^2 + cx^3$"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B18F7965B2F041318EDFE5C5F0F36506"},"cell_type":"code","source":["import org.apache.spark.mllib.linalg.Vectors\n","import org.apache.spark.mllib.regression.LabeledPoint"],"outputs":[]},{"metadata":{"id":"C90FABE6E65443538EEED719DA97CA57"},"cell_type":"markdown","source":"We define a type for the function that generates the values."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"FE31938C4EF04391B78068F5F3CBA060"},"cell_type":"code","source":["type Gen=((Double,Double))=>LabeledPoint"],"outputs":[]},{"metadata":{"id":"68CA3BD6C37141398893C3904D3B6321"},"cell_type":"markdown","source":"**Definition** of the generator for: $y=intercept$"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"EDA619024E7B41F98DAECA8FA8D2D4A1"},"cell_type":"code","source":["val toPoints0:Gen = { case (x: Double, y:Double) => LabeledPoint(y, Vectors.dense(Array(1.0))) }"],"outputs":[]},{"metadata":{"id":"68CA3BD6C37141398893C3904D3B6321"},"cell_type":"markdown","source":"**Definition** of the generator for: $y=intercept + ax$"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"21565C58803041EB97708EC0E4D1C626"},"cell_type":"code","source":["val toPoints1:Gen = { case (x: Double, y:Double) => LabeledPoint(y, Vectors.dense(Array(x, 1.0))) }"],"outputs":[]},{"metadata":{"id":"68CA3BD6C37141398893C3904D3B6321"},"cell_type":"markdown","source":"**Definition** of the generator for: $y=intercept + ax + bx^2$"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"BB0BA564A25A4EE1896FC68F9CECA7AC"},"cell_type":"code","source":["val toPoints2:Gen = { case (x: Double, y:Double) => LabeledPoint(y, Vectors.dense(Array(x*x, x, 1.0))) }"],"outputs":[]},{"metadata":{"id":"68CA3BD6C37141398893C3904D3B6321"},"cell_type":"markdown","source":"**Definition** of the generator for: $y=intercept + ax + bx^2 + cx^3$"},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false,"id":"1C4589C28E8C4661B4FCF8750704783E"},"cell_type":"code","source":["val toPoints3:Gen = { case (x: Double, y:Double) => LabeledPoint(y, Vectors.dense(Array(x*x*x, x*x, x, 1.0))) }"],"outputs":[]},{"metadata":{"id":"AACDA0B1E0034ACA9278CECDDB2F058E"},"cell_type":"markdown","source":"# Example: simple linear regression on one sample"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"EE216076D1524104A869EF8A8F0A9EBC"},"cell_type":"markdown","source":"## Take a sample of this universe (1% items)"},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false,"id":"A1967B91AAD74120B76342010A74F209"},"cell_type":"code","source":["val sample = () => universe.sample(true, 0.01)"],"outputs":[]},{"metadata":{"id":"E4910EED914E41C9865147F46AF82A5B"},"cell_type":"markdown","source":"Let's create a order `1` model this sample."},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false,"id":"8F88C6C9C4C7481085709A8440934DD5"},"cell_type":"code","source":["val data = sample().map(toPoints1)\n","LineChart(data.map(lp => (lp.features(0), lp.label)).collect.toList)"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"5C5DDFA0A89D4204A81198A9A3D29D7E"},"cell_type":"markdown","source":"## Order `1` Linear regression"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"74CC83BCECE2440F87B98803C6AA9328"},"cell_type":"code","source":["import org.apache.spark.mllib.optimization.{LBFGS, LeastSquaresGradient, SimpleUpdater}\n","import org.apache.spark.mllib.regression.LinearRegressionModel"],"outputs":[]},{"metadata":{"id":"FF283593A56349A0A28D03D1B6DC8749"},"cell_type":"markdown","source":"We're now creating a function that will basically train a **Linear Regression** using **Least-squared loss** function on the modeled data."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"948D47AD0031439780C5EEBA64828388"},"cell_type":"code","source":["val train = (dta: RDD[LabeledPoint]) => {\n","  val one = dta.first\n","  val numCorrections = 10\n","  val convergenceTol = 1e-4\n","  val maxNumIterations = 100\n","  val regParam = 0.1\n","  val initialWeightsWithIntercept = Vectors.dense(new Array[Double](one.features.size))\n","  \n","  val (weightsWithIntercept, loss) = LBFGS.runLBFGS(\n","    dta.map(lp => (lp.label, lp.features)), // create the RDD[(Double, Vector)]\n","    new LeastSquaresGradient(),             // loss function\n","    new SimpleUpdater(),\n","    numCorrections,\n","    convergenceTol,\n","    maxNumIterations,\n","    regParam,\n","    initialWeightsWithIntercept\n","  )\n","  \n","  new LinearRegressionModel(weightsWithIntercept, 0.0)\n","}"],"outputs":[]},{"metadata":{"id":"A4DE5C12B6434DDC897C47CAB27052C2"},"cell_type":"markdown","source":"Now we can run it on the first order modeled data."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"31311F89C02447DE803F0FAAF3BBABB0"},"cell_type":"code","source":["val model = train(data)"],"outputs":[]},{"metadata":{"id":"5BFC0FCB8A7249DDAE0650D41DC295B8"},"cell_type":"markdown","source":"## Estimate the error for the `1` order model"},{"metadata":{"id":"36ABBE07C9B04B88848CDD1CF89CE1E9"},"cell_type":"markdown","source":"This is going to apply the model on the sample data we have, then it computes the squared error on it.\n\nRecall that this dataset is **exactly** the same as for the training part."},{"metadata":{"id":"04F0937EDD594CB0B4AFA52A2BECE816"},"cell_type":"markdown","source":"First we define the way we'll want to plot the data to \"visualize\" the performance using the\n- features\n- label\n- predictions"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B6EB4D6C52954360965E9F8E7FAC414D"},"cell_type":"code","source":["import org.apache.spark.mllib.linalg.Vector\n","def plotResults(features:RDD[Vector], labels:RDD[Double], predictions:RDD[Double]) = {\n","  val featuresData = features.map(f => f.toArray.sum)\n","  val realData = sparkSession.createDataFrame(featuresData zip labels).withColumn(\"type\", lit(\"original\"))\n","  val predData = sparkSession.createDataFrame(featuresData zip predictions).withColumn(\"type\", lit(\"prediction\"))\n","  \n","  new CustomPlotlyChart((realData union predData).orderBy(\"_1\"),\n","                        layout=\"{title: 'Plot real and predicted values'}\",\n","                        dataOptions=\"\"\"{\n","                          splitBy: 'type',\n","                          byTrace: {\n","                            'real': {\n","                              mode: 'lines+markers',\n","                              marker: {\n","                                color: 'rgb(219, 64, 82)',\n","                                size: 4\n","                              }\n","                            },\n","                            'pred': {\n","                              mode: 'markers',\n","                              line: {\n","                                color: 'rgb(55, 128, 191)',\n","                                width: 2\n","                              }\n","                            }\n","                          }\n","                        }\"\"\",\n","                        dataSources=\"{x: '_1', y: '_2'}\") \n","}"],"outputs":[]},{"metadata":{"id":"2E0A0BFA19B041ECA11CAA05DA279C00"},"cell_type":"markdown","source":"Then we can create a function to compute the predictions and **mean squared error**."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"D5CD3D9B33C54816921B2B194D72868E"},"cell_type":"code","source":["def run(model: LinearRegressionModel, dta: RDD[LabeledPoint]) = new {\n","  // local copies to avoid serialization problem\n","  val localModel = model\n","  val localLabel = dta.map(d => d.label)\n","  val localFeatures = dta.map(d => d.features)\n","  \n","  // we zip the label (true values) with predicted values using the `model.predict` function\n","  val localPredictions = localModel.predict(localFeatures)\n","  val valuesAndPreds = localLabel zip localPredictions\n","    \n","  // we compute the Mean Square Error\n","  val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()  \n","  \n","  val plot = plotResults(localFeatures, localLabel, localPredictions)\n","}"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B5937C71B1154E3C8B141F3995D9ED16"},"cell_type":"code","source":["val results0 = run(model, data)"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"65B8BECDA7754B4784509D6FB0D8BF20"},"cell_type":"code","source":["results0.MSE"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"6CB551205453495192890885E95495CF"},"cell_type":"code","source":["results0.plot"],"outputs":[]},{"metadata":{"id":"FB67DDEE258048EB83ED2A89FB88BE89"},"cell_type":"markdown","source":"## How does this predict works?"},{"metadata":{"id":"2D59689F671645E5827CBB8B60134B9D"},"cell_type":"markdown","source":"Getting back to the model, we basically trained two parameters:\n* $\\beta_0$, the interceptor\n* $\\beta_1$, the weigths vector (slope in the dimensions) "},{"metadata":{"id":"338A48C427694EFA90515844123844CA"},"cell_type":"markdown","source":"Hence the prediction will take the features we have seen, the data, and apply a linear inference on it, that is apply $y = \\beta_0 + \\beta_1 * x$ "},{"metadata":{"id":"74899D1AE8F0485E8B87FA2F46E816D7"},"cell_type":"markdown","source":"We'll use the intercept and weights below"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab2025578243-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"721A2BEAD89444A280D8743386A3736F"},"cell_type":"code","source":["List((\"intercept\", model.intercept), (\"weights\", model.weights))"],"outputs":[]},{"metadata":{"id":"7C11DFB7FDF141C688263AF6EA9D8F4A"},"cell_type":"markdown","source":"So let's take an observation, and look at the features (data) and label (value)."},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false,"id":"CC96A77EF9F14422B60D7422D1953C44"},"cell_type":"code","source":["val sample1 = data.first"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab1190184474-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"40F08FE2742C4058A3923452B5A511E9"},"cell_type":"code","source":["List((\"label\", sample1.label), (\"features\", sample1.features))"],"outputs":[]},{"metadata":{"id":"390D8C977E7B4A70A2DE6D10FEA9F74F"},"cell_type":"markdown","source":"In the following, we'll apply the linear regression manually by executing the polynomial combination using the\n- intercept\n- features values `times` the trained weights"},{"metadata":{"id":"32CA9BF907BB43EB8D8F587CE07DA4EC"},"cell_type":"markdown","source":"$$ \n\\begin{aligned}\npredict &= intercept + features * weights \\\\\n        &= intercept + \\sum_{i}^{} features_i * weights_i\n\\end{aligned}\n$$"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"15542711F2F944F483C0292CB887FAFE"},"cell_type":"code","source":["val psum = (sample1.features.toArray zip model.weights.toArray) // list of pairs (feature, weight)\n","            .map(x => x._1*x._2)                                // list of products (equation term)\n","            .sum                                                // sum\n","val predict1 = model.intercept + psum"],"outputs":[]},{"metadata":{"id":"CCE1FDC9BD3D43998CA8EB2D8002EB53"},"cell_type":"markdown","source":"We can check how far this prediction is from correct label:"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"FCBF7983181F4CD38C546D37539AB941"},"cell_type":"code","source":["math.abs(sample1.label - predict1)"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"8543A76674EC46DE8398BBCC0F9064D4"},"cell_type":"markdown","source":"# Try 0, 1st, 2nd & 3rd order functions on 1% universe samples"},{"metadata":{"id":"A118979CC53F44388DE694D82EB445FB"},"cell_type":"markdown","source":"## Simulations"},{"metadata":{"id":"0B253DBF0F764355BCFD8CBA379715C9"},"cell_type":"markdown","source":"Now we will run a lot of times the models on training data but also keep some data for validation testing later, that are usually named testing datasets.\n\nSo that we are simulating cases where we have a lot of samples that where either taken from the same dataset, or have been taken at different time."},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false,"id":"1AB46BBC5C354E83BBEB0C70F5747ADB"},"cell_type":"code","source":["case class ModelsMSE(mses: Array[(notebook.front.Widget, Double)])\n","\n","val errors = (1 to 20).map{ i =>\n","                            \n","  val data = universe.sample(true, 0.01)                            \n","  val data0 = data.map(toPoints0)\n","  val data1 = data.map(toPoints1)\n","  val data2 = data.map(toPoints2)\n","  val data3 = data.map(toPoints3)\n","  \n","  val test = universe\n","  val test0 = test.map(toPoints0)\n","  val test1 = test.map(toPoints1)\n","  val test2 = test.map(toPoints2)\n","  val test3 = test.map(toPoints3)\n","              \n","  val model0 = train(data0)\n","  val model1 = train(data1)\n","  val model2 = train(data2)\n","  val model3 = train(data3)\n","\n","  new {\n","    val run0 = (run(model0, data0), run(model0, test0))\n","    val run1 = (run(model1, data1), run(model1, test1))\n","    val run2 = (run(model2, data2), run(model2, test2))\n","    val run3 = (run(model3, data3), run(model3, test3))\n","  }\n","}"],"outputs":[]},{"metadata":{"id":"DA982911753047D8BF4050F3D061BBB1"},"cell_type":"markdown","source":"Let's take a look at **one** of these runs to see how trainings and tests predictions are comparable"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"4434E5F3CD8C42988F39947E2083F37C"},"cell_type":"code","source":["val picked = scala.util.Random.shuffle(errors).head\n","\n","<div>\n","<h2>Picked run</h2>\n","<h3>Order 0</h3>\n","<h4>Training set: mse {picked.run0._1.MSE}</h4>\n","{picked.run0._1.plot}\n","<h4>Test set: mse {picked.run0._2.MSE}</h4>\n","{picked.run0._2.plot}\n","\n","<h3>Order 1</h3>\n","<h4>Training set: mse {picked.run1._1.MSE}</h4>\n","{picked.run1._1.plot}\n","<h4>Test set: mse {picked.run1._2.MSE}</h4>\n","{picked.run1._2.plot}\n","\n","<h3>Order 2</h3>\n","<h4>Training set: mse {picked.run2._1.MSE}</h4>\n","{picked.run2._1.plot}\n","<h4>Test set: mse {picked.run2._2.MSE}</h4>\n","{picked.run2._2.plot}\n","\n","<h3>Order 3</h3>\n","<h4>Training set: mse {picked.run3._1.MSE}</h4>\n","{picked.run3._1.plot}\n","<h4>Test set: mse {picked.run3._2.MSE}</h4>\n","{picked.run3._2.plot}\n","</div>\n"],"outputs":[]},{"metadata":{"id":"D38840AC5E9C4B6CBFEABF23F9927B3A"},"cell_type":"markdown","source":"## Comparing the models"},{"metadata":{"id":"1C2CD2C012C342EB962492190AB5A242"},"cell_type":"markdown","source":"In the following, we'll be using the 20 trainings and testing we've done to check which model won.\n\nFor this, we'll count the number of times a specific order worked better than the others, for both the training set and the testing set."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"869D8EBD88314C5292EB9A79366A087D"},"cell_type":"code","source":["class ErrorSummary(order: Int, var bestCount: Int, var sse: Double, var count: Int) {\n","  def addBest(se: Double) = {\n","    bestCount = bestCount+1\n","    add(se)\n","  }\n","  def add(se: Double) = {\n","    sse       = sse+se\n","    count     = count+1\n","  }\n","  \n","  override def toString() = s\"$order, $bestCount, $sse, $count\"\n","}"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B893FEC322DF4F7292070FF1CFB67A61"},"cell_type":"code","source":["val zeros = List.tabulate(4)(i => Error(i, 0, 0.0, 0))\n","val errorTests = Map(\n","  0 -> new ErrorSummary(0, 0, 0.0, 0),\n","  1 -> new ErrorSummary(1, 0, 0.0, 0),\n","  2 -> new ErrorSummary(2, 0, 0.0, 0),\n","  3 -> new ErrorSummary(3, 0, 0.0, 0)\n",")\n","val errorsTrainings = Map(\n","  0 -> new ErrorSummary(0, 0, 0.0, 0),\n","  1 -> new ErrorSummary(1, 0, 0.0, 0),\n","  2 -> new ErrorSummary(2, 0, 0.0, 0),\n","  3 -> new ErrorSummary(3, 0, 0.0, 0)\n",")"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab53391727-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"254086C4497D4C8D94C39B45CF02E4A6"},"cell_type":"code","source":["errors.foreach { error =>\n","  val runs = List(error.run0, error.run1, error.run2, error.run3)\n","  val (trains, tests) = {\n","    val (trains, tests) = runs.unzip\n","    (trains.zipWithIndex, tests.zipWithIndex)\n","  }\n","  val bestTrain = trains.minBy(_._1.MSE)\n","  val bestTest  = tests.minBy(_._1.MSE)\n","  \n","  (0 until 4).foreach { i =>\n","    if (i == bestTrain._2) {\n","      errorsTrainings(i).addBest(bestTrain._1.MSE)\n","    } else {\n","      errorsTrainings(i).add(runs(i)._1.MSE)\n","    }\n","    if (i == bestTest._2) {\n","      errorTests(i).addBest(bestTest._1.MSE)\n","    } else {\n","      errorTests(i).add(runs(i)._2.MSE)\n","    }\n","  }\n","}"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab1966097651-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"A74DDB99A1BA4CD883F4B93967C4BCF3"},"cell_type":"code","source":["errorTests"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"81D9B41A811A4BA4A88E3088776F09BB"},"cell_type":"code","source":["table(\n","  5,\n","  Seq.tabulate(4){i=>\n","    Seq(\n","      text(\"\"+i), text(\"\"+{errorTests(i).bestCount}), text(\"\"+(errorTests(i).sse/errorTests(i).count)), \n","      text(\"\"+errorsTrainings(i).bestCount), text(\"\"+(errorsTrainings(i).sse/errorsTrainings(i).count))\n","    )\n","  }.flatten,\n","  Seq(text(\"Model Order\"), text(\"# Best model\"), text(\"Mean Squared Error\"), text(\"Training best model\"), text(\"Training MSE\"))\n",")"],"outputs":[]},{"metadata":{"id":"D4D06BB634A14B5EBFB3F42F58EA30E0"},"cell_type":"markdown","source":"As we can see, the **second order** model is more often picked on the **test** data sets, which is great because we **know** (we're lucky) that the **Universe** is actually quadratic.\n\nHowever, if we look at the **training** data set best models, we can see that the **third** order is/should **always** be the best. We is countersense with our knwowledge of the Universe.\nThis is simply because, the $R^2$ can only be reduced by adding new component into the model.\n\nThat's also why we be better use some regularization to choose the model than relying on a single metric.\n\nBut that's a different story!"}],"nbformat":4}